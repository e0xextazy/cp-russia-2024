{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = 'output_test'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex = True\n",
    "    print_freq = 100\n",
    "    num_workers = 8\n",
    "    model = \"ai-forever/ru-en-RoSBERTa\"\n",
    "    gradient_checkpointing = False\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    epochs = 10\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 8\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    # target_cols = ['Классификатор 1 уровня']\n",
    "    seed = 42\n",
    "    n_fold = 6\n",
    "    trn_fold = [0]\n",
    "    train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"Классификатор 1 уровня\"] = df[\"Классификатор 1 уровня\"].apply(lambda x: x.strip())\n",
    "    df[\"Классификатор 2 уровня\"] = df[\"Классификатор 2 уровня\"].apply(lambda x: x.strip())\n",
    "    # df[\"text\"] = df[\"text\"].astype(str)\n",
    "    # df[\"class\"] = df[\"class\"].astype(str)\n",
    "    # df.drop_duplicates([\"class\", \"text\"], inplace=True)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(\n",
    "    #     re.findall(r\"[а-яА-Я0-9 ёЁ\\-\\.,?!+a-zA-Z]+\", x)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_score(y_trues, class1_predictions, class2_predictions):\n",
    "    class1_predictions = [np.argmax(el) for el in class1_predictions]\n",
    "    class2_predictions = [np.argmax(el) for el in class2_predictions]\n",
    "\n",
    "    class1_score = accuracy_score(y_trues[:, 0], class1_predictions)\n",
    "    class2_score = accuracy_score(y_trues[:, 1], class2_predictions)\n",
    "    return class1_score, class2_score\n",
    "\n",
    "\n",
    "def get_logger(filename=os.path.join(OUTPUT_DIR, 'train')):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 6)\n",
      "(801, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel(\"../case/02_Реальные_кейсы.xlsx\")\n",
    "print(train.shape)\n",
    "train = preprocess(train)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_le = LabelEncoder()\n",
    "class2_le = LabelEncoder()\n",
    "class1_le.fit(train[\"Классификатор 1 уровня\"].tolist())\n",
    "class2_le.fit(train[\"Классификатор 2 уровня\"].tolist())\n",
    "train[\"Классификатор 1 уровня\"] = class1_le.transform(train[\"Классификатор 1 уровня\"].tolist())\n",
    "train[\"Классификатор 2 уровня\"] = class2_le.transform(train[\"Классификатор 2 уровня\"].tolist())\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"class1_le.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(class1_le, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"class2_le.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(class2_le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Вопрос пользователя</th>\n",
       "      <th>Ответ сотрудника</th>\n",
       "      <th>Вопрос из БЗ</th>\n",
       "      <th>Ответ из БЗ</th>\n",
       "      <th>Классификатор 1 уровня</th>\n",
       "      <th>Классификатор 2 уровня</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>Добрый день!\\nЧто нельзя публиковать на RUTUBE...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Добрый вечер, какой топ причин блокировки виде...</td>\n",
       "      <td>Добрый вечер!\\nЧто заперщено публиковать на RU...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Все пишут, что монетизация на рутубе отключает...</td>\n",
       "      <td>Добрый день! \\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Что запрещено в монетизации и что можно выклад...</td>\n",
       "      <td>Здравствуйте!\\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>Для монетизации можно использовать то, что вы ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Вопрос пользователя                                   Ответ сотрудника                                       Вопрос из БЗ                                        Ответ из БЗ  Классификатор 1 уровня  Классификатор 2 уровня  fold\n",
       "0  Здравствуйте! Можно уточнить причины Правилhtt...  Добрый день!\\nЧто нельзя публиковать на RUTUBE...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...                       3                      14     3\n",
       "1  Добрый вечер, какой топ причин блокировки виде...  Добрый вечер!\\nЧто заперщено публиковать на RU...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...                       3                      14     4\n",
       "2  Все пишут, что монетизация на рутубе отключает...  Добрый день! \\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15     0\n",
       "3  Что запрещено в монетизации и что можно выклад...  Здравствуйте!\\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15     1\n",
       "4  Чтобы не отключали монетизацию, надо, чтобы я ...  Для монетизации можно использовать то, что вы ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold,\n",
    "                       shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[[\"Классификатор 1 уровня\", \"Классификатор 2 уровня\"]].values)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe939ffaab9641cbbdc7896ad8fcd760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 332\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, 'tokenizer'))\n",
    "CFG.tokenizer = tokenizer\n",
    "\n",
    "lengths = []\n",
    "tk0 = tqdm(train['Вопрос пользователя'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 2  # cls & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n",
    "CFG.max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['Вопрос пользователя'].values\n",
    "        self.class1 = df[\"Классификатор 1 уровня\"].values\n",
    "        self.class2 = df[\"Классификатор 2 уровня\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        class1 = torch.tensor(self.class1[item], dtype=torch.long)\n",
    "        class2 = torch.tensor(self.class2[item], dtype=torch.long)\n",
    "        return inputs, class1, class2\n",
    "\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states,\n",
    "                 attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def pool(hidden_state, mask, pooling_method=\"mean\"):\n",
    "    if pooling_method == \"mean\":\n",
    "        s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "        d = mask.sum(axis=1, keepdim=True).float()\n",
    "        return s / d\n",
    "    elif pooling_method == \"cls\":\n",
    "        return hidden_state[:, 0]\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.fc_class1 = nn.Linear(self.config.hidden_size, 11)\n",
    "        self.fc_class2 = nn.Linear(self.config.hidden_size, 39)\n",
    "        self._init_weights(self.fc_class1)\n",
    "        self._init_weights(self.fc_class2)\n",
    "        # self.freeze_model()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def freeze_model(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_vector_e5(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        feature = average_pool(outputs.last_hidden_state,\n",
    "                               inputs['attention_mask'])\n",
    "        return F.normalize(feature, p=2, dim=1)\n",
    "    \n",
    "    def get_vector_sbert(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def get_vector_rubert(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_vector_ruenrosberta(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = pool(\n",
    "            outputs.last_hidden_state, \n",
    "            inputs[\"attention_mask\"],\n",
    "            pooling_method=\"cls\" # or try \"mean\"\n",
    "        )\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def get_vector_labse(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        feature = outputs.pooler_output\n",
    "        return torch.nn.functional.normalize(feature)\n",
    "    \n",
    "    def head1(self, feature):\n",
    "        output_class1 = self.fc_class1(feature)\n",
    "        return output_class1\n",
    "    \n",
    "    def head2(self, feature):\n",
    "        output_class2 = self.fc_class2(feature)\n",
    "        return output_class2\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        feature = self.get_vector_ruenrosberta(inputs)\n",
    "        output_class1 = self.fc_class1(feature)\n",
    "        output_class2 = self.fc_class2(feature)\n",
    "\n",
    "        return output_class1, output_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, class1, class2) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        class1 = class1.to(device)\n",
    "        class2 = class2.to(device)\n",
    "\n",
    "        batch_size = class1.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            feature = model.get_vector_ruenrosberta(inputs)\n",
    "            pred1 = model.head1(feature)\n",
    "            pred2 = model.head2(feature)\n",
    "            loss = criterion(pred1, class1) + criterion(pred2, class2)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds1 = []\n",
    "    preds2 = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, label1, label2) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        label1 = label1.to(device)\n",
    "        label2 = label2.to(device)\n",
    "\n",
    "        batch_size = label1.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred1, pred2 = model.inference(inputs)\n",
    "            loss = criterion(pred1, label1) + criterion(pred2, label2)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds1.append(pred1.to('cpu').numpy())\n",
    "        preds2.append(pred2.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions1 = np.concatenate(preds1)\n",
    "    predictions2 = np.concatenate(preds2)\n",
    "    return losses.avg, predictions1, predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[[\"Классификатор 1 уровня\", \"Классификатор 2 уровня\"]].values\n",
    "\n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, os.path.join(OUTPUT_DIR, 'config.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr,\n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr,\n",
    "                      eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss()  # МБ добавить веса в лосс\n",
    "    best_score = -1 * float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model,\n",
    "                            criterion, optimizer, epoch, scheduler, device)\n",
    "        # eval\n",
    "        avg_val_loss, predictions1, predictions2 = valid_fn(\n",
    "            valid_loader, model, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        score1, score2 = get_score(valid_labels, predictions1, predictions2)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score1:.4f} {score2:.4f}')\n",
    "        score = score1 + score2\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions1': predictions1, 'predictions2': predictions2},\n",
    "                       os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"))\n",
    "\n",
    "    predictions1 = torch.load(os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"),\n",
    "                             map_location=torch.device('cpu'))['predictions1']\n",
    "    predictions2 = torch.load(os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"),\n",
    "                             map_location=torch.device('cpu'))['predictions2']\n",
    "\n",
    "    valid_folds[\"pred1\"] = [np.argmax(el) for el in predictions1]\n",
    "    valid_folds[\"pred2\"] = [np.argmax(el) for el in predictions2]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ru-en-RoSBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 98505\n",
      "}\n",
      "\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ru-en-RoSBERTa and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/83] Elapsed 0m 0s (remain 0m 41s) Loss: 6.0735(6.0735) Grad: 86607.2969  LR: 0.00002000  \n",
      "Epoch: [1][82/83] Elapsed 0m 7s (remain 0m 0s) Loss: 5.8489(5.9016) Grad: 70031.8047  LR: 0.00001952  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.8223(5.8223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 5.9016  avg_val_loss: 5.7980  time: 7s\n",
      "Epoch 1 - Score: 0.3383 0.2707\n",
      "Epoch 1 - Save Best Score: 0.6090 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.7256(5.7980) \n",
      "Epoch: [2][0/83] Elapsed 0m 0s (remain 0m 12s) Loss: 5.7625(5.7625) Grad: 79715.7734  LR: 0.00001950  \n",
      "Epoch: [2][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.5237(5.7112) Grad: 93331.9922  LR: 0.00001811  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.6953(5.6953) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 5.7112  avg_val_loss: 5.6346  time: 7s\n",
      "Epoch 2 - Score: 0.5865 0.3985\n",
      "Epoch 2 - Save Best Score: 0.9850 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.5645(5.6346) \n",
      "Epoch: [3][0/83] Elapsed 0m 0s (remain 0m 12s) Loss: 5.7272(5.7272) Grad: 133805.6094  LR: 0.00001809  \n",
      "Epoch: [3][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.6136(5.5482) Grad: 161845.4688  LR: 0.00001592  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.5960(5.5960) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 5.5482  avg_val_loss: 5.5325  time: 7s\n",
      "Epoch 3 - Score: 0.6466 0.4812\n",
      "Epoch 3 - Save Best Score: 1.1278 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4856(5.5325) \n",
      "Epoch: [4][0/83] Elapsed 0m 0s (remain 0m 12s) Loss: 5.6775(5.6775) Grad: 126747.8672  LR: 0.00001589  \n",
      "Epoch: [4][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.3488(5.4270) Grad: 92666.2891  LR: 0.00001316  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.5118(5.5118) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 5.4270  avg_val_loss: 5.4625  time: 7s\n",
      "Epoch 4 - Score: 0.6992 0.5414\n",
      "Epoch 4 - Save Best Score: 1.2406 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4968(5.4625) \n",
      "Epoch: [5][0/83] Elapsed 0m 0s (remain 0m 13s) Loss: 5.3000(5.3000) Grad: 118544.8984  LR: 0.00001313  \n",
      "Epoch: [5][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.3014(5.3393) Grad: 84426.1641  LR: 0.00001009  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.4727(5.4727) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 5.3393  avg_val_loss: 5.4181  time: 7s\n",
      "Epoch 5 - Score: 0.6992 0.5489\n",
      "Epoch 5 - Save Best Score: 1.2481 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4310(5.4181) \n",
      "Epoch: [6][0/83] Elapsed 0m 0s (remain 0m 11s) Loss: 5.1512(5.1512) Grad: 98528.1641  LR: 0.00001006  \n",
      "Epoch: [6][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.1544(5.2699) Grad: 103415.0078  LR: 0.00000702  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4283(5.4283) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 5.2699  avg_val_loss: 5.3810  time: 7s\n",
      "Epoch 6 - Score: 0.7368 0.5789\n",
      "Epoch 6 - Save Best Score: 1.3158 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4086(5.3810) \n",
      "Epoch: [7][0/83] Elapsed 0m 0s (remain 0m 11s) Loss: 5.1782(5.1782) Grad: 103064.5078  LR: 0.00000698  \n",
      "Epoch: [7][82/83] Elapsed 0m 6s (remain 0m 0s) Loss: 5.1736(5.2241) Grad: 60935.0000  LR: 0.00000423  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.4154(5.4154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 5.2241  avg_val_loss: 5.3552  time: 7s\n",
      "Epoch 7 - Score: 0.7444 0.5865\n",
      "Epoch 7 - Save Best Score: 1.3308 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4181(5.3552) \n",
      "Epoch: [8][0/83] Elapsed 0m 0s (remain 0m 11s) Loss: 5.1044(5.1044) Grad: 118887.6875  LR: 0.00000420  \n",
      "Epoch: [8][82/83] Elapsed 0m 7s (remain 0m 0s) Loss: 5.0591(5.1864) Grad: 76630.0547  LR: 0.00000200  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.3986(5.3986) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 5.1864  avg_val_loss: 5.3436  time: 8s\n",
      "Epoch 8 - Score: 0.7519 0.5865\n",
      "Epoch 8 - Save Best Score: 1.3383 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4261(5.3436) \n",
      "Epoch: [9][0/83] Elapsed 0m 0s (remain 0m 12s) Loss: 5.1295(5.1295) Grad: 73761.8047  LR: 0.00000198  \n",
      "Epoch: [9][82/83] Elapsed 0m 7s (remain 0m 0s) Loss: 5.1296(5.1715) Grad: 67315.6562  LR: 0.00000054  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.3920(5.3920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 5.1715  avg_val_loss: 5.3389  time: 8s\n",
      "Epoch 9 - Score: 0.7519 0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4273(5.3389) \n",
      "Epoch: [10][0/83] Elapsed 0m 0s (remain 0m 12s) Loss: 5.1799(5.1799) Grad: 75243.9922  LR: 0.00000053  \n",
      "Epoch: [10][82/83] Elapsed 0m 7s (remain 0m 0s) Loss: 5.1992(5.1679) Grad: 74673.6406  LR: 0.00000000  \n",
      "EVAL: [0/9] Elapsed 0m 0s (remain 0m 1s) Loss: 5.3911(5.3911) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 5.1679  avg_val_loss: 5.3382  time: 8s\n",
      "Epoch 10 - Score: 0.7519 0.6015\n",
      "Epoch 10 - Save Best Score: 1.3534 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [8/9] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4264(5.3382) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.7519 | 0.6015\n",
      "========== CV ==========\n",
      "Score: 0.7519 | 0.6015\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_result(oof_df):\n",
    "        label1 = oof_df[\"Классификатор 1 уровня\"].tolist()\n",
    "        label2 = oof_df[\"Классификатор 2 уровня\"].tolist()\n",
    "        predictions1 = oof_df[\"pred1\"].tolist()\n",
    "        predictions2 = oof_df[\"pred2\"].tolist()\n",
    "        score1 = accuracy_score(label1, predictions1)\n",
    "        score2 = accuracy_score(label2, predictions2)\n",
    "        LOGGER.info(f'Score: {score1:.4f} | {score2:.4f}')\n",
    "\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(os.path.join(OUTPUT_DIR, 'oof_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
