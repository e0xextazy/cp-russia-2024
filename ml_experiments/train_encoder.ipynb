{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = 'output_ruenrosberta_v5'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex = True\n",
    "    print_freq = 100\n",
    "    num_workers = 8\n",
    "    model = \"ai-forever/ru-en-RoSBERTa\"\n",
    "    gradient_checkpointing = False\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    epochs = 10\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 16\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    # target_cols = ['Классификатор 1 уровня']\n",
    "    seed = 42\n",
    "    n_fold = 6\n",
    "    trn_fold = [0]\n",
    "    train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"Классификатор 1 уровня\"] = df[\"Классификатор 1 уровня\"].apply(\n",
    "        lambda x: x.strip())\n",
    "    df[\"Классификатор 2 уровня\"] = df[\"Классификатор 2 уровня\"].apply(\n",
    "        lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_score(y_trues, class1_predictions, class2_predictions):\n",
    "    class1_predictions = [np.argmax(el) for el in class1_predictions]\n",
    "    class2_predictions = [np.argmax(el) for el in class2_predictions]\n",
    "\n",
    "    class1_score = accuracy_score(y_trues[:, 0], class1_predictions)\n",
    "    class2_score = accuracy_score(y_trues[:, 1], class2_predictions)\n",
    "    return class1_score, class2_score\n",
    "\n",
    "\n",
    "def get_logger(filename=os.path.join(OUTPUT_DIR, 'train')):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Вопрос пользователя</th>\n",
       "      <th>Ответ сотрудника</th>\n",
       "      <th>Вопрос из БЗ</th>\n",
       "      <th>Ответ из БЗ</th>\n",
       "      <th>Классификатор 1 уровня</th>\n",
       "      <th>Классификатор 2 уровня</th>\n",
       "      <th>aug_questions</th>\n",
       "      <th>Paraphrase 1</th>\n",
       "      <th>Paraphrase 2</th>\n",
       "      <th>Paraphrase 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>Добрый день!\\nЧто нельзя публиковать на RUTUBE...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>МОДЕРАЦИЯ</td>\n",
       "      <td>Отклонение/блокировка видео</td>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>Добрый день! Подскажите, по каким причинам ваш...</td>\n",
       "      <td>Здравствуйте! Не могли бы вы разъяснить, за чт...</td>\n",
       "      <td>Приветствую, хочу уточнить, на каком основании...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Добрый вечер, какой топ причин блокировки виде...</td>\n",
       "      <td>Добрый вечер!\\nЧто заперщено публиковать на RU...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>МОДЕРАЦИЯ</td>\n",
       "      <td>Отклонение/блокировка видео</td>\n",
       "      <td>Добрый вечер, какой топ основ блокировки видео...</td>\n",
       "      <td>1. Здравствуйте, подскажите, пожалуйста, топ п...</td>\n",
       "      <td>2. Привет, можно узнать, какие основные причин...</td>\n",
       "      <td>3. Добрый день, какие самые частые причины уда...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Все пишут, что монетизация на рутубе отключает...</td>\n",
       "      <td>Добрый день! \\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>МОНЕТИЗАЦИЯ</td>\n",
       "      <td>Отключение/подключение монетизации</td>\n",
       "      <td>Все изображают, что монетизация на рутубе откл...</td>\n",
       "      <td>1. Читаю, что на Рутубе монетизацию само по се...</td>\n",
       "      <td>2. Все говорят, что на Рутубе монетизацию само...</td>\n",
       "      <td>3. Многие пишут, что на Рутубе монетизация сам...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Что запрещено в монетизации и что можно выклад...</td>\n",
       "      <td>Здравствуйте!\\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>МОНЕТИЗАЦИЯ</td>\n",
       "      <td>Отключение/подключение монетизации</td>\n",
       "      <td>Что запрещено в монетизации и что можно мостить?</td>\n",
       "      <td>1. \"Какие ограничения на монетизацию и что раз...</td>\n",
       "      <td>2. \"Что нельзя монетизировать и что можно публ...</td>\n",
       "      <td>3. \"Какие материалы запрещено монетизировать и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>Для монетизации можно использовать то, что вы ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>МОНЕТИЗАЦИЯ</td>\n",
       "      <td>Отключение/подключение монетизации</td>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>1. Так, чтобы мне не вырубили монету, это мне ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2. Если не хочу, чтобы меня отключили от монет...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                Вопрос пользователя                                   Ответ сотрудника                                       Вопрос из БЗ                                        Ответ из БЗ Классификатор 1 уровня              Классификатор 2 уровня                                      aug_questions                                       Paraphrase 1                                       Paraphrase 2                                       Paraphrase 3\n",
       "0           0  Здравствуйте! Можно уточнить причины Правилhtt...  Добрый день!\\nЧто нельзя публиковать на RUTUBE...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...              МОДЕРАЦИЯ         Отклонение/блокировка видео  Здравствуйте! Можно уточнить причины Правилhtt...  Добрый день! Подскажите, по каким причинам ваш...  Здравствуйте! Не могли бы вы разъяснить, за чт...  Приветствую, хочу уточнить, на каком основании...\n",
       "1           1  Добрый вечер, какой топ причин блокировки виде...  Добрый вечер!\\nЧто заперщено публиковать на RU...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...              МОДЕРАЦИЯ         Отклонение/блокировка видео  Добрый вечер, какой топ основ блокировки видео...  1. Здравствуйте, подскажите, пожалуйста, топ п...  2. Привет, можно узнать, какие основные причин...  3. Добрый день, какие самые частые причины уда...\n",
       "2           2  Все пишут, что монетизация на рутубе отключает...  Добрый день! \\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...            МОНЕТИЗАЦИЯ  Отключение/подключение монетизации  Все изображают, что монетизация на рутубе откл...  1. Читаю, что на Рутубе монетизацию само по се...  2. Все говорят, что на Рутубе монетизацию само...  3. Многие пишут, что на Рутубе монетизация сам...\n",
       "3           3  Что запрещено в монетизации и что можно выклад...  Здравствуйте!\\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...            МОНЕТИЗАЦИЯ  Отключение/подключение монетизации   Что запрещено в монетизации и что можно мостить?  1. \"Какие ограничения на монетизацию и что раз...  2. \"Что нельзя монетизировать и что можно публ...  3. \"Какие материалы запрещено монетизировать и...\n",
       "4           4  Чтобы не отключали монетизацию, надо, чтобы я ...  Для монетизации можно использовать то, что вы ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...            МОНЕТИЗАЦИЯ  Отключение/подключение монетизации  Чтобы не отключали монетизацию, надо, чтобы я ...  1. Так, чтобы мне не вырубили монету, это мне ...                                                NaN  2. Если не хочу, чтобы меня отключили от монет..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../case/Aug_data (1).csv\",\n",
    "                    sep=\"|\").rename(columns={\"q\": \"Вопрос пользователя\"})\n",
    "train = preprocess(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_le = LabelEncoder()\n",
    "class2_le = LabelEncoder()\n",
    "class1_le.fit(train[\"Классификатор 1 уровня\"].tolist())\n",
    "class2_le.fit(train[\"Классификатор 2 уровня\"].tolist())\n",
    "train[\"Классификатор 1 уровня\"] = class1_le.transform(\n",
    "    train[\"Классификатор 1 уровня\"].tolist())\n",
    "train[\"Классификатор 2 уровня\"] = class2_le.transform(\n",
    "    train[\"Классификатор 2 уровня\"].tolist())\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"class1_le.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(class1_le, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"class2_le.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(class2_le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Вопрос пользователя</th>\n",
       "      <th>Ответ сотрудника</th>\n",
       "      <th>Вопрос из БЗ</th>\n",
       "      <th>Ответ из БЗ</th>\n",
       "      <th>Классификатор 1 уровня</th>\n",
       "      <th>Классификатор 2 уровня</th>\n",
       "      <th>aug_questions</th>\n",
       "      <th>Paraphrase 1</th>\n",
       "      <th>Paraphrase 2</th>\n",
       "      <th>Paraphrase 3</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>Добрый день!\\nЧто нельзя публиковать на RUTUBE...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>Добрый день! Подскажите, по каким причинам ваш...</td>\n",
       "      <td>Здравствуйте! Не могли бы вы разъяснить, за чт...</td>\n",
       "      <td>Приветствую, хочу уточнить, на каком основании...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Добрый вечер, какой топ причин блокировки виде...</td>\n",
       "      <td>Добрый вечер!\\nЧто заперщено публиковать на RU...</td>\n",
       "      <td>Что нельзя публиковать на RUTUBE?</td>\n",
       "      <td>Чужой контент без разрешения автора или правоо...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>Добрый вечер, какой топ основ блокировки видео...</td>\n",
       "      <td>1. Здравствуйте, подскажите, пожалуйста, топ п...</td>\n",
       "      <td>2. Привет, можно узнать, какие основные причин...</td>\n",
       "      <td>3. Добрый день, какие самые частые причины уда...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Все пишут, что монетизация на рутубе отключает...</td>\n",
       "      <td>Добрый день! \\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>Все изображают, что монетизация на рутубе откл...</td>\n",
       "      <td>1. Читаю, что на Рутубе монетизацию само по се...</td>\n",
       "      <td>2. Все говорят, что на Рутубе монетизацию само...</td>\n",
       "      <td>3. Многие пишут, что на Рутубе монетизация сам...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Что запрещено в монетизации и что можно выклад...</td>\n",
       "      <td>Здравствуйте!\\nМонетизация может отключиться, ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>Что запрещено в монетизации и что можно мостить?</td>\n",
       "      <td>1. \"Какие ограничения на монетизацию и что раз...</td>\n",
       "      <td>2. \"Что нельзя монетизировать и что можно публ...</td>\n",
       "      <td>3. \"Какие материалы запрещено монетизировать и...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>Для монетизации можно использовать то, что вы ...</td>\n",
       "      <td>Почему могут отключить монетизацию из-за автор...</td>\n",
       "      <td>Монетизация может отключиться, если на вашем к...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>1. Так, чтобы мне не вырубили монету, это мне ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2. Если не хочу, чтобы меня отключили от монет...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                Вопрос пользователя                                   Ответ сотрудника                                       Вопрос из БЗ                                        Ответ из БЗ  Классификатор 1 уровня  Классификатор 2 уровня                                      aug_questions                                       Paraphrase 1                                       Paraphrase 2                                       Paraphrase 3  fold\n",
       "0           0  Здравствуйте! Можно уточнить причины Правилhtt...  Добрый день!\\nЧто нельзя публиковать на RUTUBE...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...                       3                      14  Здравствуйте! Можно уточнить причины Правилhtt...  Добрый день! Подскажите, по каким причинам ваш...  Здравствуйте! Не могли бы вы разъяснить, за чт...  Приветствую, хочу уточнить, на каком основании...     3\n",
       "1           1  Добрый вечер, какой топ причин блокировки виде...  Добрый вечер!\\nЧто заперщено публиковать на RU...                  Что нельзя публиковать на RUTUBE?  Чужой контент без разрешения автора или правоо...                       3                      14  Добрый вечер, какой топ основ блокировки видео...  1. Здравствуйте, подскажите, пожалуйста, топ п...  2. Привет, можно узнать, какие основные причин...  3. Добрый день, какие самые частые причины уда...     4\n",
       "2           2  Все пишут, что монетизация на рутубе отключает...  Добрый день! \\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15  Все изображают, что монетизация на рутубе откл...  1. Читаю, что на Рутубе монетизацию само по се...  2. Все говорят, что на Рутубе монетизацию само...  3. Многие пишут, что на Рутубе монетизация сам...     0\n",
       "3           3  Что запрещено в монетизации и что можно выклад...  Здравствуйте!\\nМонетизация может отключиться, ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15   Что запрещено в монетизации и что можно мостить?  1. \"Какие ограничения на монетизацию и что раз...  2. \"Что нельзя монетизировать и что можно публ...  3. \"Какие материалы запрещено монетизировать и...     1\n",
       "4           4  Чтобы не отключали монетизацию, надо, чтобы я ...  Для монетизации можно использовать то, что вы ...  Почему могут отключить монетизацию из-за автор...  Монетизация может отключиться, если на вашем к...                       4                      15  Чтобы не отключали монетизацию, надо, чтобы я ...  1. Так, чтобы мне не вырубили монету, это мне ...                                                NaN  2. Если не хочу, чтобы меня отключили от монет...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold,\n",
    "                                 shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[[\"Классификатор 1 уровня\", \"Классификатор 2 уровня\"]].values)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4141, 4)\n",
      "(3827, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Вопрос пользователя</th>\n",
       "      <th>Классификатор 1 уровня</th>\n",
       "      <th>Классификатор 2 уровня</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Здравствуйте! Можно уточнить причины Правилhtt...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Добрый вечер, какой топ причин блокировки виде...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Все пишут, что монетизация на рутубе отключает...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Что запрещено в монетизации и что можно выклад...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чтобы не отключали монетизацию, надо, чтобы я ...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Вопрос пользователя  Классификатор 1 уровня  Классификатор 2 уровня  fold\n",
       "0  Здравствуйте! Можно уточнить причины Правилhtt...                       3                      14     3\n",
       "1  Добрый вечер, какой топ причин блокировки виде...                       3                      14     4\n",
       "2  Все пишут, что монетизация на рутубе отключает...                       4                      15     0\n",
       "3  Что запрещено в монетизации и что можно выклад...                       4                      15     1\n",
       "4  Чтобы не отключали монетизацию, надо, чтобы я ...                       4                      15     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = train[[\"Вопрос пользователя\", \"Классификатор 1 уровня\",\n",
    "               \"Классификатор 2 уровня\", \"fold\"]]\n",
    "extra1 = train[[\"aug_questions\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\",\n",
    "                \"fold\"]].rename(columns={\"aug_questions\": \"Вопрос пользователя\"})\n",
    "extra1 = extra1[extra1.fold != 0]\n",
    "extra2 = train[[\"Paraphrase 1\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\",\n",
    "                \"fold\"]].rename(columns={\"Paraphrase 1\": \"Вопрос пользователя\"})\n",
    "extra2 = extra2[extra2.fold != 0]\n",
    "extra3 = train[[\"Paraphrase 2\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\",\n",
    "                \"fold\"]].rename(columns={\"Paraphrase 2\": \"Вопрос пользователя\"})\n",
    "extra3 = extra3[extra3.fold != 0]\n",
    "extra4 = train[[\"Paraphrase 3\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\",\n",
    "                \"fold\"]].rename(columns={\"Paraphrase 3\": \"Вопрос пользователя\"})\n",
    "extra4 = extra4[extra4.fold != 0]\n",
    "extra5 = train[[\"Вопрос из БЗ\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\",\n",
    "                \"fold\"]].rename(columns={\"Вопрос из БЗ\": \"Вопрос пользователя\"})\n",
    "extra5 = extra5[extra5.fold != 0]\n",
    "\n",
    "train = pd.concat([extra, extra1, extra2, extra3, extra4, extra5], axis=0)\n",
    "\n",
    "print(train.shape)\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d19fd558c714035b2287e54084f55ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 332\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, 'tokenizer'))\n",
    "CFG.tokenizer = tokenizer\n",
    "\n",
    "lengths = []\n",
    "tk0 = tqdm(train['Вопрос пользователя'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 2  # cls & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n",
    "CFG.max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['Вопрос пользователя'].values\n",
    "        self.class1 = df[\"Классификатор 1 уровня\"].values\n",
    "        self.class2 = df[\"Классификатор 2 уровня\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        class1 = torch.tensor(self.class1[item], dtype=torch.long)\n",
    "        class2 = torch.tensor(self.class2[item], dtype=torch.long)\n",
    "        return inputs, class1, class2\n",
    "\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states,\n",
    "                 attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(\n",
    "        ~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def pool(hidden_state, mask, pooling_method=\"mean\"):\n",
    "    if pooling_method == \"mean\":\n",
    "        s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "        d = mask.sum(axis=1, keepdim=True).float()\n",
    "        return s / d\n",
    "    elif pooling_method == \"cls\":\n",
    "        return hidden_state[:, 0]\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # First element of model_output contains all token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(\n",
    "        -1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.fc_class1 = nn.Linear(self.config.hidden_size, 11)\n",
    "        self.fc_class2 = nn.Linear(self.config.hidden_size, 39)\n",
    "        self._init_weights(self.fc_class1)\n",
    "        self._init_weights(self.fc_class2)\n",
    "        # self.freeze_model()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def freeze_model(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_vector_e5(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        feature = average_pool(outputs.last_hidden_state,\n",
    "                               inputs['attention_mask'])\n",
    "        return F.normalize(feature, p=2, dim=1)\n",
    "\n",
    "    def get_vector_sbert(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def get_vector_rubert(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def get_vector_ruenrosberta(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = pool(\n",
    "            outputs.last_hidden_state,\n",
    "            inputs[\"attention_mask\"],\n",
    "            pooling_method=\"cls\"  # or try \"mean\"\n",
    "        )\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    def get_vector_labse(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        feature = outputs.pooler_output\n",
    "        return torch.nn.functional.normalize(feature)\n",
    "\n",
    "    def head1(self, feature):\n",
    "        output_class1 = self.fc_class1(feature)\n",
    "        return output_class1\n",
    "\n",
    "    def head2(self, feature):\n",
    "        output_class2 = self.fc_class2(feature)\n",
    "        return output_class2\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        feature = self.get_vector_ruenrosberta(inputs)\n",
    "        output_class1 = self.fc_class1(feature)\n",
    "        output_class2 = self.fc_class2(feature)\n",
    "\n",
    "        return output_class1, output_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, class1, class2) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        class1 = class1.to(device)\n",
    "        class2 = class2.to(device)\n",
    "\n",
    "        batch_size = class1.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            feature = model.get_vector_ruenrosberta(inputs)\n",
    "            pred1 = model.head1(feature)\n",
    "            pred2 = model.head2(feature)\n",
    "            loss = criterion(pred1, class1) + criterion(pred2, class2)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds1 = []\n",
    "    preds2 = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, label1, label2) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        label1 = label1.to(device)\n",
    "        label2 = label2.to(device)\n",
    "\n",
    "        batch_size = label1.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred1, pred2 = model.inference(inputs)\n",
    "            loss = criterion(pred1, label1) + criterion(pred2, label2)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds1.append(pred1.to('cpu').numpy())\n",
    "        preds2.append(pred2.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions1 = np.concatenate(preds1)\n",
    "    predictions2 = np.concatenate(preds2)\n",
    "    return losses.avg, predictions1, predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[[\n",
    "        \"Классификатор 1 уровня\", \"Классификатор 2 уровня\"]].values\n",
    "\n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, os.path.join(OUTPUT_DIR, 'config.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr,\n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr,\n",
    "                      eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss()  # МБ добавить веса в лосс\n",
    "    best_score = -1 * float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model,\n",
    "                            criterion, optimizer, epoch, scheduler, device)\n",
    "        # eval\n",
    "        avg_val_loss, predictions1, predictions2 = valid_fn(\n",
    "            valid_loader, model, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        score1, score2 = get_score(valid_labels, predictions1, predictions2)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score1:.4f} {score2:.4f}')\n",
    "        score = score1 + score2\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions1': predictions1, 'predictions2': predictions2},\n",
    "                       os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"))\n",
    "\n",
    "    predictions1 = torch.load(os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"),\n",
    "                              map_location=torch.device('cpu'))['predictions1']\n",
    "    predictions2 = torch.load(os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"),\n",
    "                              map_location=torch.device('cpu'))['predictions2']\n",
    "\n",
    "    valid_folds[\"pred1\"] = [np.argmax(el) for el in predictions1]\n",
    "    valid_folds[\"pred2\"] = [np.argmax(el) for el in predictions2]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ru-en-RoSBERTa\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 98505\n",
      "}\n",
      "\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ru-en-RoSBERTa and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/230] Elapsed 0m 0s (remain 1m 57s) Loss: 6.0719(6.0719) Grad: 61510.6133  LR: 0.00002000  \n",
      "Epoch: [1][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 5.6578(5.8359) Grad: 81974.0859  LR: 0.00001991  \n",
      "Epoch: [1][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 5.4834(5.7177) Grad: 136981.3438  LR: 0.00001963  \n",
      "Epoch: [1][229/230] Elapsed 0m 22s (remain 0m 0s) Loss: 5.3924(5.6874) Grad: 105478.2344  LR: 0.00001951  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.5552(5.5552) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 5.6874  avg_val_loss: 5.4943  time: 23s\n",
      "Epoch 1 - Score: 0.6767 0.4737\n",
      "Epoch 1 - Save Best Score: 1.1504 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.4313(5.4943) \n",
      "Epoch: [2][0/230] Elapsed 0m 0s (remain 0m 40s) Loss: 5.5051(5.5051) Grad: 76648.4609  LR: 0.00001951  \n",
      "Epoch: [2][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 5.4131(5.3933) Grad: 104708.7188  LR: 0.00001900  \n",
      "Epoch: [2][200/230] Elapsed 0m 18s (remain 0m 2s) Loss: 5.3361(5.3449) Grad: 317840.6875  LR: 0.00001833  \n",
      "Epoch: [2][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 5.1277(5.3287) Grad: 108802.2422  LR: 0.00001810  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.3489(5.3489) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 5.3287  avg_val_loss: 5.3186  time: 22s\n",
      "Epoch 2 - Score: 0.7594 0.5940\n",
      "Epoch 2 - Save Best Score: 1.3534 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.3515(5.3186) \n",
      "Epoch: [3][0/230] Elapsed 0m 0s (remain 0m 34s) Loss: 5.2231(5.2231) Grad: 104428.5859  LR: 0.00001809  \n",
      "Epoch: [3][100/230] Elapsed 0m 9s (remain 0m 11s) Loss: 5.0740(5.1577) Grad: 199979.0156  LR: 0.00001722  \n",
      "Epoch: [3][200/230] Elapsed 0m 18s (remain 0m 2s) Loss: 5.0098(5.1163) Grad: 167139.6250  LR: 0.00001622  \n",
      "Epoch: [3][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 4.9102(5.1032) Grad: 76861.9297  LR: 0.00001590  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.1590(5.1590) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 5.1032  avg_val_loss: 5.1799  time: 22s\n",
      "Epoch 3 - Score: 0.7594 0.5639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.2410(5.1799) \n",
      "Epoch: [4][0/230] Elapsed 0m 0s (remain 0m 41s) Loss: 5.1265(5.1265) Grad: 216815.0156  LR: 0.00001589  \n",
      "Epoch: [4][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.8769(4.9567) Grad: 78048.0234  LR: 0.00001474  \n",
      "Epoch: [4][200/230] Elapsed 0m 18s (remain 0m 2s) Loss: 4.8368(4.9142) Grad: 59605.2070  LR: 0.00001350  \n",
      "Epoch: [4][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 4.7867(4.9044) Grad: 128772.4297  LR: 0.00001313  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.0661(5.0661) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 4.9044  avg_val_loss: 5.0374  time: 22s\n",
      "Epoch 4 - Score: 0.7519 0.6241\n",
      "Epoch 4 - Save Best Score: 1.3759 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.1270(5.0374) \n",
      "Epoch: [5][0/230] Elapsed 0m 0s (remain 0m 42s) Loss: 4.7936(4.7936) Grad: 79577.0156  LR: 0.00001312  \n",
      "Epoch: [5][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.7937(4.7706) Grad: 160910.6094  LR: 0.00001180  \n",
      "Epoch: [5][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 4.6931(4.7441) Grad: 52378.8867  LR: 0.00001045  \n",
      "Epoch: [5][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 4.7269(4.7353) Grad: 69871.0469  LR: 0.00001005  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.9227(4.9227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 4.7353  avg_val_loss: 4.9228  time: 23s\n",
      "Epoch 5 - Score: 0.7820 0.6466\n",
      "Epoch 5 - Save Best Score: 1.4286 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 5.0709(4.9228) \n",
      "Epoch: [6][0/230] Elapsed 0m 0s (remain 0m 36s) Loss: 4.5443(4.5443) Grad: 83182.7031  LR: 0.00001004  \n",
      "Epoch: [6][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.8084(4.6260) Grad: 64716.3789  LR: 0.00000868  \n",
      "Epoch: [6][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 4.5928(4.6084) Grad: 57777.9766  LR: 0.00000735  \n",
      "Epoch: [6][229/230] Elapsed 0m 22s (remain 0m 0s) Loss: 4.5278(4.6034) Grad: 45234.0586  LR: 0.00000697  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.8365(4.8365) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 4.6034  avg_val_loss: 4.8255  time: 23s\n",
      "Epoch 6 - Score: 0.8045 0.6316\n",
      "Epoch 6 - Save Best Score: 1.4361 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.8978(4.8255) \n",
      "Epoch: [7][0/230] Elapsed 0m 0s (remain 0m 36s) Loss: 4.5581(4.5581) Grad: 158933.9219  LR: 0.00000696  \n",
      "Epoch: [7][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.4415(4.5304) Grad: 48723.6484  LR: 0.00000569  \n",
      "Epoch: [7][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 4.4839(4.5177) Grad: 47865.9297  LR: 0.00000451  \n",
      "Epoch: [7][229/230] Elapsed 0m 22s (remain 0m 0s) Loss: 4.4926(4.5120) Grad: 44889.8320  LR: 0.00000418  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7690(4.7690) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 4.5120  avg_val_loss: 4.7504  time: 23s\n",
      "Epoch 7 - Score: 0.8195 0.6466\n",
      "Epoch 7 - Save Best Score: 1.4662 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7897(4.7504) \n",
      "Epoch: [8][0/230] Elapsed 0m 0s (remain 0m 34s) Loss: 4.4472(4.4472) Grad: 44923.5195  LR: 0.00000417  \n",
      "Epoch: [8][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.5313(4.4657) Grad: 41413.5820  LR: 0.00000312  \n",
      "Epoch: [8][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 4.3001(4.4581) Grad: 59820.3789  LR: 0.00000220  \n",
      "Epoch: [8][229/230] Elapsed 0m 22s (remain 0m 0s) Loss: 4.4944(4.4581) Grad: 44697.4766  LR: 0.00000196  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7474(4.7474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 4.4581  avg_val_loss: 4.7298  time: 23s\n",
      "Epoch 8 - Score: 0.8120 0.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7860(4.7298) \n",
      "Epoch: [9][0/230] Elapsed 0m 0s (remain 0m 37s) Loss: 4.4007(4.4007) Grad: 44667.5664  LR: 0.00000195  \n",
      "Epoch: [9][100/230] Elapsed 0m 9s (remain 0m 12s) Loss: 4.6258(4.4344) Grad: 49686.5859  LR: 0.00000122  \n",
      "Epoch: [9][200/230] Elapsed 0m 19s (remain 0m 2s) Loss: 4.3477(4.4303) Grad: 42844.9648  LR: 0.00000065  \n",
      "Epoch: [9][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 4.4947(4.4329) Grad: 43337.5039  LR: 0.00000052  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7422(4.7422) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 4.4329  avg_val_loss: 4.7211  time: 23s\n",
      "Epoch 9 - Score: 0.8045 0.6466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7681(4.7211) \n",
      "Epoch: [10][0/230] Elapsed 0m 0s (remain 0m 43s) Loss: 4.5139(4.5139) Grad: 46239.8750  LR: 0.00000052  \n",
      "Epoch: [10][100/230] Elapsed 0m 9s (remain 0m 11s) Loss: 4.3133(4.4316) Grad: 48490.3711  LR: 0.00000017  \n",
      "Epoch: [10][200/230] Elapsed 0m 18s (remain 0m 2s) Loss: 4.4323(4.4285) Grad: 47637.1016  LR: 0.00000001  \n",
      "Epoch: [10][229/230] Elapsed 0m 21s (remain 0m 0s) Loss: 4.3848(4.4256) Grad: 54693.8125  LR: 0.00000000  \n",
      "EVAL: [0/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7408(4.7408) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 4.4256  avg_val_loss: 4.7195  time: 22s\n",
      "Epoch 10 - Score: 0.8045 0.6391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [4/5] Elapsed 0m 0s (remain 0m 0s) Loss: 4.7692(4.7195) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.8195 | 0.6466\n",
      "========== CV ==========\n",
      "Score: 0.8195 | 0.6466\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_result(oof_df):\n",
    "        label1 = oof_df[\"Классификатор 1 уровня\"].tolist()\n",
    "        label2 = oof_df[\"Классификатор 2 уровня\"].tolist()\n",
    "        predictions1 = oof_df[\"pred1\"].tolist()\n",
    "        predictions2 = oof_df[\"pred2\"].tolist()\n",
    "        score1 = accuracy_score(label1, predictions1)\n",
    "        score2 = accuracy_score(label2, predictions2)\n",
    "        LOGGER.info(f'Score: {score1:.4f} | {score2:.4f}')\n",
    "\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(os.path.join(OUTPUT_DIR, 'oof_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
