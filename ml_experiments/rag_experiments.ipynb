{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pool(hidden_state, mask, pooling_method=\"csl\"): # переиспользовать\n",
    "#     if pooling_method == \"mean\":\n",
    "#         s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "#         d = mask.sum(axis=1, keepdim=True).float()\n",
    "#         return s / d\n",
    "#     elif pooling_method == \"cls\":\n",
    "#         return hidden_state[:, 0]\n",
    "\n",
    "\n",
    "# def prepare_input(text): # переиспользовать\n",
    "#     inputs = tokenizer.encode_plus(\n",
    "#         text,\n",
    "#         return_tensors=None,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=512,\n",
    "#         pad_to_max_length=True,\n",
    "#         truncation=True,\n",
    "#     )\n",
    "#     for k, v in inputs.items():\n",
    "#         inputs[k] = torch.tensor([v], dtype=torch.long)\n",
    "#     return inputs\n",
    "\n",
    "# def get_vector(text): # переиспользовать\n",
    "#     inputs = prepare_input(text)\n",
    "#     outputs = model(**inputs)\n",
    "#     embeddings = pool(\n",
    "#             outputs.last_hidden_state,\n",
    "#             inputs[\"attention_mask\"],\n",
    "#             pooling_method=\"cls\" # or try \"mean\"\n",
    "#         )\n",
    "#     return F.normalize(embeddings, p=2, dim=1).tolist()[0]\n",
    "\n",
    "# def nodes_from_tables():\n",
    "#     nodes = []\n",
    "#     df = pd.read_excel(\"../case/01_База_знаний.xlsx\").sample(n=10)\n",
    "#     df.drop(columns=[\"Классификатор 1 уровня\", \"Классификатор 2 уровня\", \"Тема\"], inplace=True)\n",
    "\n",
    "#     for i, row in df.iterrows():\n",
    "#         node = TextNode(text=row[\"Ответ из БЗ\"], embedding=get_vector(row[\"Вопрос из БЗ\"]))\n",
    "#         nodes.append(node)\n",
    "\n",
    "#     df = pd.read_excel(\"../case/02_Реальные_кейсы.xlsx\").sample(n=10)\n",
    "#     df.drop(columns=[\"Вопрос из БЗ\", \"Ответ из БЗ\", \"Классификатор 1 уровня\", \"Классификатор 2 уровня\"], inplace=True)\n",
    "\n",
    "#     for i, row in df.iterrows():\n",
    "#         node = TextNode(text=row[\"Ответ сотрудника\"], embedding=get_vector(row[\"Вопрос пользователя\"]))\n",
    "#         nodes.append(node)\n",
    "\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../api/encoder_model. Creating a new one with mean pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ru-en-RoSBERTa and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2358b6c6dea04cb789a2b7b180a2624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=[\n",
    "                             \"../case/03_ГЕНЕРАЛЬНОЕ ПОЛЬЗОВАТЕЛЬСКОЕ СОГЛАШЕНИЕ RUTUBE.docx\", \"../case/04_УСЛОВИЯ РАЗМЕЩЕНИЯ КОНТЕНТА.docx\"]).load_data()\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "for node in nodes:\n",
    "    node.text = re.sub(\"\\\\n\", \" \", node.text)\n",
    "    node.text = re.sub(\" +\", \" \", node.text)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"../api/encoder_model\"\n",
    ")\n",
    "# model_name = \"ai-forever/ru-en-RoSBERTa\"\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_llama3_8b\"\n",
    "\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model=model_llm,\n",
    "    tokenizer=tokenizer_llm,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.00001,\n",
    "        \"do_sample\": False,\n",
    "        \"repetition_penalty\": 1.12,\n",
    "        # \"top_p\": 0.9,\n",
    "        # \"top_k\": 30,\n",
    "        \"pad_token_id\": 128000,\n",
    "        \"eos_token_id\": 128009,\n",
    "        \"bos_token_id\": 128000},\n",
    "    system_prompt=\"Ты умный ассистент которого зовут Наташа. Ты любишь отвечать на вопросы пользователей.\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "# table_nodes = nodes_from_tables()\n",
    "# index.insert_nodes(table_nodes)\n",
    "\n",
    "vector_query_engine = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-05` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/home/pc/Desktop/cp-russia-2024/venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    \"Ниже представлена информация из правовых документов:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"На основе вышеуказанной информации из документов, ответь на вопрос пользователя. В ответе укажи только содержательную часть без повторения вопроса.\\n\"\n",
    "    \"Вопрос: {query}\\n\"\n",
    "    \"Ответ:\"\n",
    ")\n",
    "\n",
    "query = \"Где индексируется размещенный контент?\"\n",
    "# query = \"Как тебя зовут?\"\n",
    "nodes = vector_query_engine.retrieve(query)\n",
    "context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "response = llm.complete(\n",
    "    template.format(context_str=context_str, query=query)\n",
    ")\n",
    "response_clear = re.findall(\n",
    "    r\"[а-яА-Я\\n \\.\\,\\(\\)a-zA-Z\\!\\?\\/\\:0-9\\\"\\'\\;\\-\\—\\-]+(?=---)\", response.text)[0]\n",
    "response_clear = response_clear.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индексирование размещенного контента происходит в глобальных поисковых системах, таких как Google, Bing, Yandex и другие, в результате чего результаты индексирования могут отображаться в поисковой выдаче этих систем в виде ссылок на контентное содержимое канала пользователя. Это происходит автоматически, без участия администрации RUTUBE, и является естественным процессом работы глобальной сети Интернет. Браузеры, такие как Internet Explorer, Mozilla, Google Chrome, Opera и др., кэшируют контент, который был ранее просмотрен пользователем, что позволяет быстро открывать уже просматриваемые страницы. Однако это не связано с незаконным использованием объектов интеллектуальной собственности. Кэширование производится браузером в автоматическом режиме в соответствии с определенными алгоритмами.\n"
     ]
    }
   ],
   "source": [
    "print(response_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
